#!/usr/bin/env python3
import json
import os
from pathlib import Path
from typing import TypedDict, List, Dict, Optional, Any
from langgraph.graph import StateGraph, END
from groq import Groq
import requests
from dotenv import load_dotenv
import base64

load_dotenv()

class AIState(TypedDict):
    files: List[str]
    components: Dict[str, str]
    relationships: List[Dict]
    mermaid_code: str

class AIMermaidAgent:
    def __init__(self):
        """Initialize the agent with LLM and cache."""
        self.llm = Groq(api_key=os.getenv("GROQ_API_KEY"))
        self.model = "llama-3.1-8b-instant"
        self.cache = {}
        self.graph = self._build_graph()
    
    def _build_graph(self) -> Any:
        """Build the processing workflow graph."""
        workflow = StateGraph(AIState)
        
        workflow.add_node("scan", self._ai_scanner_agent)
        workflow.add_node("detect", self._ai_detector_agent)
        workflow.add_node("relate", self._ai_relationship_agent)
        workflow.add_node("mermaid", self._ai_mermaid_agent)
        
        workflow.set_entry_point("scan")
        workflow.add_edge("scan", "detect")
        workflow.add_edge("detect", "relate")
        workflow.add_edge("relate", "mermaid")
        workflow.add_edge("mermaid", END)
        
        return workflow.compile()
    
    def _ai_scanner_agent(self, state: AIState) -> Dict:
        """AI Agent 1: File Scanner"""
        print("üîç AI Agent 1: File Scanner")
        
        file_data = []
        
        for file_path in state["files"]:
            try:
                with open(file_path, 'r') as f:
                    content = f.read()
                
                if len(content.strip()) < 10:
                    continue
                
                # AI Analysis of file content
                ai_analysis = self._analyze_file_with_ai(file_path, content)
                
                file_data.append({
                    'path': file_path,
                    'content': content,
                    'ai_analysis': ai_analysis
                })
                
            except Exception as e:
                print(f"Error reading {file_path}: {e}")
                continue
        
        return {"files": file_data}
    
    def _analyze_file_with_ai(self, file_path: str, content: str) -> dict:
        """Use AI to analyze file content"""
        try:
            cache_key = f"scan_{file_path}_{len(content)}"
            if cache_key in self.cache:
                return self.cache[cache_key]
            
            ext = Path(file_path).suffix.lower()
            lang = {
                '.py': 'Python', '.js': 'JavaScript', '.ts': 'TypeScript',
                '.java': 'Java', '.go': 'Go', '.rb': 'Ruby'
            }.get(ext, 'Unknown')
            
            # Clean content to remove control characters
            clean_content = ''.join(char for char in content[:1000] if ord(char) >= 32 or char in '\n\r\t')
            
            prompt = f"""Analyze this {lang} code file and return ONLY a JSON object:

File: {file_path}
Content: {clean_content}

Return JSON:
{{
    "type": "frontend|backend|service|database|cache|queue|auth|gateway",
    "framework": "detected framework or null",
    "patterns": ["list of patterns found"]
}}"""
            
            response = self.llm.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model=self.model,
                temperature=0.0,
                max_tokens=300,
                seed=42
            )
            
            result = response.choices[0].message.content.strip()
            
            # Extract JSON more robustly
            start = result.find('{')
            if start == -1:
                return {}
            
            # Find matching closing brace
            brace_count = 0
            end = start
            for i, char in enumerate(result[start:], start):
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        end = i + 1
                        break
            
            if end > start:
                try:
                    json_str = result[start:end]
                    # Clean JSON string of control characters
                    clean_json = ''.join(char for char in json_str if ord(char) >= 32 or char in '\n\r\t')
                    ai_result = json.loads(clean_json)
                    self.cache[cache_key] = ai_result
                    return ai_result
                except json.JSONDecodeError:
                    pass
                
        except Exception as e:
            print(f"AI file analysis failed for {file_path}: {e}")
        
        return {}
    
    def _ai_detector_agent(self, state: AIState) -> Dict:
        """AI Agent 2: Component Detector"""
        print("ü§ñ AI Agent 2: Component Detector")
        
        try:
            components = {}
            file_list = state.get("files", [])
            
            if not file_list:
                return {"components": {}}
            
            # Use AI to detect components
            for file_data in file_list:
                try:
                    file_path = file_data.get('path', '')
                    content = file_data.get('content', '')
                    ai_analysis = file_data.get('ai_analysis', {})
                    
                    if not file_path:
                        continue
                    
                    # Get AI-detected component type
                    component_type = self._detect_component_with_ai(file_path, content, ai_analysis)
                    
                    if component_type:
                        path = Path(file_path)
                        parent = path.parent.name
                        name = path.stem
                        
                        component_name = (
                            f"{parent}_{name}"
                            if parent != 'complex_project'
                            else name
                        )
                        components[component_name] = component_type
                    
                except Exception as e:
                    print(f"Error processing file {file_path}: {e}")
            
            # Add implicit components for complex_project
            project = state.get('project_path', '')
            if ('complex_project' in str(project) or 
                any('complex_project' in str(f.get('path', '')) for f in file_list)):
                for dir_name in ['cache', 'database', 'queue', 'frontend']:
                    if dir_name not in [c.split('_')[0] for c in components]:
                        components[f"{dir_name}_layer"] = dir_name
            
            return {"components": components}
            
        except Exception as e:
            print(f"Component detection failed: {e}")
            return {"components": {}}
    
    def _detect_component_with_ai(self, file_path: str, content: str, ai_analysis: dict) -> str:
        """Use AI to detect component type"""
        try:
            cache_key = f"detect_{file_path}_{len(content)}"
            if cache_key in self.cache:
                return self.cache[cache_key]
            
            path = Path(file_path)
            
            # Clean content to remove control characters
            clean_content = ''.join(char for char in content[:800] if ord(char) >= 32 or char in '\n\r\t')
            
            prompt = f"""Analyze this code and determine its architectural component type.

File: {file_path}
Directory: {path.parent.name}
Previous Analysis: {ai_analysis}
Code: {clean_content}

Component Types:
- gateway: API gateways, routing
- auth: Authentication, authorization
- service: Business logic, microservices
- database: Data storage, repositories
- cache: Caching layers
- queue: Message queues, async processing
- frontend: UI, web interfaces

Return ONLY a JSON object:
{{
    "component": "gateway|auth|service|database|cache|queue|frontend",
    "confidence": 0.9
}}"""
            
            response = self.llm.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model=self.model,
                temperature=0.0,
                max_tokens=200,
                seed=42
            )
            
            result = response.choices[0].message.content.strip()
            
            # Extract JSON robustly
            start = result.find('{')
            if start == -1:
                return self._fallback_component_detection(file_path)
            
            brace_count = 0
            end = start
            for i, char in enumerate(result[start:], start):
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        end = i + 1
                        break
            
            if end > start:
                try:
                    json_str = result[start:end]
                    # Clean JSON string of control characters
                    clean_json = ''.join(char for char in json_str if ord(char) >= 32 or char in '\n\r\t')
                    ai_result = json.loads(clean_json)
                    if ai_result.get('confidence', 0) > 0.5:
                        component_type = ai_result['component']
                        self.cache[cache_key] = component_type
                        return component_type
                except json.JSONDecodeError:
                    pass
                    
        except Exception as e:
            print(f"AI component detection failed for {file_path}: {e}")
        
        return self._fallback_component_detection(file_path)
    
    def _fallback_component_detection(self, file_path: str) -> str:
        """Fallback component detection using filename patterns"""
        path = Path(file_path)
        name = path.stem.lower()
        
        if 'gateway' in name or 'api' in name:
            return 'gateway'
        elif 'auth' in name:
            return 'auth'
        elif any(x in name for x in ['database', 'db', 'repository']):
            return 'database'
        elif 'cache' in name:
            return 'cache'
        elif 'queue' in name:
            return 'queue'
        elif 'frontend' in path.parts or 'ui' in path.parts:
            return 'frontend'
        else:
            return 'service'
    
    def _ai_relationship_agent(self, state: AIState) -> Dict:
        """AI Agent 3: Relationship Finder"""
        print("üîó AI Agent 3: Relationship Finder")
        
        try:
            components = state.get("components", {})
            if not components:
                return {"relationships": []}
            
            # Use AI to find relationships
            relationships = self._find_relationships_with_ai(components)
            
            # If AI fails, use fallback logic
            if not relationships:
                relationships = self._fallback_relationships(components)
            
            return {"relationships": relationships}
        
        except Exception as e:
            print(f"Relationship detection failed: {e}")
            return {"relationships": []}
    
    def _find_relationships_with_ai(self, components: dict) -> list:
        """Use AI to find component relationships"""
        try:
            cache_key = f"relationships_{hash(str(sorted(components.items())))}"
            if cache_key in self.cache:
                return self.cache[cache_key]
            
            prompt = f"""Analyze these architectural components and find their relationships.

Components: {json.dumps(components, indent=2)}

Return ONLY a JSON array of relationships:
[
    {{
        "from": "component_name",
        "to": "component_name", 
        "type": "routes|calls|uses|publishes|authenticates"
    }}
]

Rules:
- Gateways route to services
- Frontends call services
- Services use databases/caches
- Services publish to queues
- Services authenticate via auth services"""
            
            response = self.llm.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model=self.model,
                temperature=0.0,
                max_tokens=800,
                seed=42
            )
            
            result = response.choices[0].message.content.strip()
            
            # Extract JSON array robustly
            start = result.find('[')
            if start == -1:
                return []
            
            bracket_count = 0
            end = start
            for i, char in enumerate(result[start:], start):
                if char == '[':
                    bracket_count += 1
                elif char == ']':
                    bracket_count -= 1
                    if bracket_count == 0:
                        end = i + 1
                        break
            
            if end > start:
                try:
                    json_str = result[start:end]
                    # Clean JSON string of control characters
                    clean_json = ''.join(char for char in json_str if ord(char) >= 32 or char in '\n\r\t')
                    relationships = json.loads(clean_json)
                except json.JSONDecodeError:
                    return []
                
                # Validate relationships
                valid_relationships = []
                for rel in relationships:
                    if (isinstance(rel, dict) and 
                        'from' in rel and 'to' in rel and 'type' in rel and
                        rel['from'] in components and rel['to'] in components):
                        valid_relationships.append(rel)
                
                if valid_relationships:
                    self.cache[cache_key] = valid_relationships
                    return valid_relationships
                    
        except Exception as e:
            print(f"AI relationship detection failed: {e}")
        
        return []
    
    def _fallback_relationships(self, components: dict) -> list:
        """Fallback relationship detection using rules"""
        relationships = []
        
        # Group components by type
        gateways = [name for name, type_ in components.items() if type_ == 'gateway']
        services = [name for name, type_ in components.items() if type_ in ['service', 'api', 'worker']]
        databases = [name for name, type_ in components.items() if type_ == 'database']
        caches = [name for name, type_ in components.items() if type_ == 'cache']
        queues = [name for name, type_ in components.items() if type_ == 'queue']
        auth_services = [name for name, type_ in components.items() if type_ == 'auth']
        frontends = [name for name, type_ in components.items() if type_ == 'frontend']
        
        # Add relationships
        for gateway in gateways:
            for service in services + auth_services:
                relationships.append({
                    'from': gateway,
                    'to': service,
                    'type': 'routes'
                })
        
        for frontend in frontends:
            for service in services + auth_services:
                relationships.append({
                    'from': frontend,
                    'to': service,
                    'type': 'calls'
                })
        
        for service in services + auth_services:
            for database in databases:
                relationships.append({
                    'from': service,
                    'to': database,
                    'type': 'uses'
                })
            
            for cache in caches:
                relationships.append({
                    'from': service,
                    'to': cache,
                    'type': 'uses'
                })
            
            for queue in queues:
                relationships.append({
                    'from': service,
                    'to': queue,
                    'type': 'publishes'
                })
        
        # Auth relationships
        for auth in auth_services:
            for service in services:
                if service != auth:
                    relationships.append({
                        'from': service,
                        'to': auth,
                        'type': 'authenticates'
                    })
        
        return relationships
    
    def _ai_mermaid_agent(self, state: AIState) -> Dict:
        """AI Agent 4: Mermaid Generator"""
        print("üé® AI Agent 4: Mermaid Generator")
        
        try:
            components = state.get("components", {})
            relationships = state.get("relationships", [])
            
            # Use AI to generate Mermaid code
            mermaid_code = self._generate_mermaid_with_ai(components, relationships)
            
            # If AI fails, use fallback
            if not mermaid_code or len(mermaid_code) < 20:
                mermaid_code = self._fallback_mermaid_generation(components, relationships)
            
            # Save to file
            with open('ai_architecture.mmd', 'w') as f:
                f.write(mermaid_code)
            
            return {"mermaid_code": mermaid_code}
        
        except Exception as e:
            print(f"Mermaid generation failed: {e}")
            return {"mermaid_code": "flowchart TB\n    A[Error] --> B[Failed to generate diagram]"}
    
    def _generate_mermaid_with_ai(self, components: dict, relationships: list) -> str:
        """Use AI to generate Mermaid diagram code"""
        try:
            cache_key = f"mermaid_{hash(str(sorted(components.items())))}_{hash(str(relationships))}"
            if cache_key in self.cache:
                return self.cache[cache_key]
            
            prompt = f"""Generate a clean Mermaid flowchart diagram for this architecture.

Components: {json.dumps(components, indent=2)}
Relationships: {json.dumps(relationships, indent=2)}

Requirements:
- Use "flowchart TB" direction
- Group components into subgraphs by layer (Gateway_Layer, Frontend_Layer, Service_Layer, Data_Layer)
- Use simple arrows: -->
- Clean node names with quotes

Return ONLY the Mermaid code, no explanations."""
            
            response = self.llm.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model=self.model,
                temperature=0.0,
                max_tokens=1000,
                seed=42
            )
            
            result = response.choices[0].message.content.strip()
            
            # Clean up the result
            if "```mermaid" in result:
                result = result.split("```mermaid")[1].split("```")[0].strip()
            elif "```" in result:
                result = result.split("```")[1].strip()
            
            # Validate it starts with flowchart
            if result.startswith("flowchart"):
                self.cache[cache_key] = result
                return result
                
        except Exception as e:
            print(f"AI Mermaid generation failed: {e}")
        
        return ""
    
    def _fallback_mermaid_generation(self, components: dict, relationships: list) -> str:
        """Fallback Mermaid generation using templates"""
        lines = ["flowchart TB"]
        
        # Group components by type
        gateways = [name for name, type_ in components.items() if type_ == 'gateway']
        services = [name for name, type_ in components.items() if type_ in ['service', 'auth']]
        data = [name for name, type_ in components.items() if type_ in ['database', 'cache', 'queue']]
        frontends = [name for name, type_ in components.items() if type_ == 'frontend']
        
        # Add subgraphs
        if gateways:
            lines.append("    subgraph Gateway_Layer")
            for comp in gateways:
                lines.append(f'        {comp}["{comp}"]')
            lines.append("    end")
        
        if frontends:
            lines.append("    subgraph Frontend_Layer")
            for comp in frontends:
                lines.append(f'        {comp}["{comp}"]')
            lines.append("    end")
        
        if services:
            lines.append("    subgraph Service_Layer")
            for comp in services:
                lines.append(f'        {comp}["{comp}"]')
            lines.append("    end")
        
        if data:
            lines.append("    subgraph Data_Layer")
            for comp in data:
                lines.append(f'        {comp}["{comp}"]')
            lines.append("    end")
        
        # Add relationships
        for rel in relationships:
            if 'from' in rel and 'to' in rel:
                lines.append(f'    {rel["from"]} --> {rel["to"]}')
        
        return '\n'.join(lines)
    
    def _convert_to_png(self, mermaid_code):
        """Convert Mermaid to PNG using online service"""
        try:
            encoded = base64.b64encode(mermaid_code.encode('utf-8')).decode('ascii')
            url = f"https://mermaid.ink/img/{encoded}?backgroundColor=white&theme=default"
            
            response = requests.get(url, timeout=30)
            if response.status_code == 200:
                with open('ai_architecture.png', 'wb') as f:
                    f.write(response.content)
                print("‚úÖ AI-generated PNG: ai_architecture.png")
            else:
                print(f"‚ùå PNG conversion failed: Status {response.status_code}")
        except Exception as e:
            print(f"‚ùå PNG conversion error: {e}")
    
    def analyze(self, code_path):
        """Run the AI-powered analysis"""
        print("üöÄ AI-Powered Mermaid Analysis")
        print("ü§ñ Each LangGraph node is an AI agent")
        print("=" * 50)
        
        # Get Python files
        files = [str(f) for f in Path(code_path).rglob("*.py")]
        
        # Run the AI agent workflow
        result = self.graph.invoke({
            "files": files,
            "components": {},
            "relationships": [],
            "mermaid_code": "",
            "project_path": code_path
        })
        
        # Save results
        with open('ai_analysis.json', 'w') as f:
            json.dump({
                'components': result['components'],
                'relationships': result['relationships'],
                'mermaid_code': result['mermaid_code']
            }, f, indent=2)
        
        # Convert to PNG
        self._convert_to_png(result['mermaid_code'])
        
        print("\n" + "=" * 50)
        print("‚úÖ AI ANALYSIS COMPLETE!")
        print("=" * 50)
        print(f"üìä Components: {len(result['components'])}")
        print(f"üîó Relationships: {len(result['relationships'])}")
        print("\nüìÑ Generated Files:")
        print("   üé® ai_architecture.mmd - AI-generated Mermaid")
        print("   üñºÔ∏è  ai_architecture.png - AI-generated diagram")
        print("   üìä ai_analysis.json - AI analysis data")
        
        return result

def main():
    import sys
    code_path = sys.argv[1] if len(sys.argv) > 1 else "sample_project"
    
    if not os.getenv("GROQ_API_KEY"):
        print("‚ùå Please set GROQ_API_KEY environment variable")
        return
    
    agent = AIMermaidAgent()
    agent.analyze(code_path)

if __name__ == "__main__":
    main()